{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a531b44",
      "metadata": {
        "id": "8a531b44"
      },
      "outputs": [],
      "source": [
        "# This script finds all duplicate resources and their parent (item) or children (asset) in Academic Commons (AC).\n",
        "#\n",
        "# First created: 2022-11-10\n",
        "# Finalized: 2022-11-22\n",
        "# 2nd Edition: 2022-12-22\n",
        "#\n",
        "# Main Processes:\n",
        "# 1) Import (a) the complete data exported from AC, and (b) a list of current duplicates in AC.\n",
        "# 2) Select items from (b) that are marked as duplicates ('Yes dupe').\n",
        "# 3) Look up bulk AC data for dupllicates' child assets.\n",
        "# 4) Output as 2 CSV, one for Hyacinth, the other one for DataCite. See descriptions below.\n",
        "#\n",
        "# 2nd Edition:\n",
        "# To merge stats of duplicate items and assets on Hyacinth, mapping duplicate child assets to its stay-published equivalent.\n",
        "# It should be the first child asset that is:\n",
        "# (a) Published on AC ('Publish Target 1 > String Key' = academic_commons), and\n",
        "# (b) 'Asset Data > Original Filename' is not 'mets.xml', which should never be published.\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f281dc5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "f281dc5e",
        "outputId": "007e2d5a-7b10-423d-fbe4-3cbfb8fb3773"
      },
      "outputs": [],
      "source": [
        "# **Import the AC exported full dataset**\n",
        "\n",
        "df= pd.read_csv('ac_export_data.csv', dtype='string')\n",
        "\n",
        "# df.head()      # Sample data\n",
        "# print(df.columns.tolist())     # Total 8092 columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871ccafc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# **Import the PID list from \"Duplicates in AC\" to compare with the main list.**\n",
        "\n",
        "currentACDupe = pd.read_csv('currentPIDList.csv')\n",
        "\n",
        "# currentACPID_list.head()       # Sample data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c03988e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract only the relevent columns from the full AC data to speed up process.\n",
        "\n",
        "trimmedACData = df[['PID', '_doi', 'Digital Object Type > String Key', 'Title 1 > Sort Portion', 'Parent Digital Object 1 > PID', 'Asset Data > Original Filename', 'Asset Data > Checksum', 'Publish Target 1 > String Key']]\n",
        "trimmedACData.columns = ['PID', 'DOI', 'Object Type', 'Title', 'Parent PID', 'Filename', 'Checksum', 'Published']\n",
        "trimmedACData = trimmedACData.drop([0])     # Remove first row of element keys.\n",
        "\n",
        "trimmedACData.head(3)        # Sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad9f5ea0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select rows that are marked as duplicates from the \"Duplicate in AC\" CSV.\n",
        "\n",
        "currentACDupe = currentACDupe[currentACDupe['YES dupe'] == True].reset_index(drop=True)\n",
        "\n",
        "# Create a list of duplicate PIDs.\n",
        "\n",
        "currentDupePID = currentACDupe['delete--PID'].tolist()\n",
        "\n",
        "currentACDupe.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ffb5a9a",
      "metadata": {
        "id": "7ffb5a9a"
      },
      "outputs": [],
      "source": [
        "# If any resource in the current dupe list is a parent item, find all of its children (other assets under that parent item).\n",
        "# These new found children will join the current dupe list to be worked on.\n",
        "# New list shows: Child index - Child PID - Child Resource Type - Child Title - Parent PID\n",
        "# A parent PID can be repeated on multiple rows if having more than one child\n",
        "\n",
        "# isin() checks each value of the currentDupePID[] if exist in the parent object column in bulk AC data.\n",
        "childofDupeParent = trimmedACData[trimmedACData['Parent PID'].isin(currentDupePID)]\n",
        "#childofDupeParent.columns = ['PID', 'DOI', 'Object Type', 'Title', 'Parent PID', 'Filename', 'Published']\n",
        "\n",
        "childofDupeParent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd1fff5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapping new parent items' PID and DOI information for the child assets found in last step.\n",
        "# The new column 'DOI to Map to' will be the DOIs that the duplicates should be redirected to.\n",
        "# Information of new DOIs comes from the current AC duplicate list.\n",
        "\n",
        "parentNewDOI = currentACDupe[['delete--PID', 'keep--PID', 'keep--DOI']]\n",
        "\n",
        "childofDupeParent = childofDupeParent.merge(parentNewDOI, how='left', left_on='Parent PID', right_on='delete--PID', sort=True)\n",
        "childofDupeParent = childofDupeParent.drop(columns=['delete--PID'])\n",
        "childofDupeParent = childofDupeParent.rename(columns={'keep--PID': 'Parent PID to Keep', 'keep--DOI': 'DOI to Map to'})\n",
        "\n",
        "childofDupeParent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57be86f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge duplicate parents' children data to currentACDupe\n",
        "# Parent and chilren are listed together\n",
        "# Resulting list contains:\n",
        "# DOI - PID - Object Type - Title - New PID - New DOI (to map to)\n",
        "\n",
        "currentACDupSubset = currentACDupe[['delete--DOI', 'delete--PID', 'OR Digital Object Type > String Key', 'OR Title 1 > Sort Portion', 'keep--PID', 'keep--DOI']]\n",
        "currentACDupSubset.columns = ['DOI', 'PID', 'Object Type', 'Title', 'Parent PID to Keep', 'DOI to Map to']\n",
        "# currentACDupSubset\n",
        "\n",
        "allItemsMapped = pd.concat([currentACDupSubset, childofDupeParent], ignore_index=True)\n",
        "\n",
        "allItemsMapped = allItemsMapped.drop(columns=['Parent PID'])\n",
        "allItemsMapped = allItemsMapped.sort_values(by='PID')\n",
        "allItemsMapped.reset_index(inplace=True, drop=True)\n",
        "allItemsMapped = allItemsMapped.replace(r'\\r', r'', regex=True)   # Remove irrelevant new lines from values\n",
        "\n",
        "# Reformat 'keep--DOI' as actual URL\n",
        "\n",
        "allItemsMapped['DOI to Map to'] = allItemsMapped['DOI to Map to'].str.replace(r'doi:', 'https://academiccommons.columbia.edu/doi/', regex=True)\n",
        "\n",
        "#allItemsMapped.to_csv('all_dupe_and_related.csv')\n",
        "allItemsMapped.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b91bfe9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2nd Edition new function: child asset level mapping\n",
        "# Mapping duplicate child assets to its stay-published equivalent for Hyacinth to merge the stats before removal.\n",
        "\n",
        "# Create a list of parent items to be kept.\n",
        "keepParentPID = allItemsMapped['Parent PID to Keep'].unique()\n",
        "keepParentPIDlist = keepParentPID.tolist()\n",
        "\n",
        "# Collect all child assets of each keeping parent.\n",
        "childofKeepParent = trimmedACData[trimmedACData['Parent PID'].isin(keepParentPIDlist)]\n",
        "\n",
        "# Select duplicate and keeping assets that are published on Academic Commons, and are not \"mets.xml\"\n",
        "selectedDupeChild = allItemsMapped[allItemsMapped['Object Type'] == 'asset']\n",
        "selectedDupeChild = selectedDupeChild[selectedDupeChild['Published'] == 'academic_commons']\n",
        "selectedDupeChild = selectedDupeChild[selectedDupeChild['Filename'] != 'mets.xml']\n",
        "\n",
        "childofKeepParent = childofKeepParent[childofKeepParent['Published'] == 'academic_commons']\n",
        "childofKeepParent = childofKeepParent[childofKeepParent['Filename'] != 'mets.xml']\n",
        "\n",
        "# Mapping duplicate child assets to its stay-published equivalent by Parent PID\n",
        "allChildMapped = selectedDupeChild.merge(childofKeepParent, how='left', left_on=['Parent PID to Keep'], right_on=['Parent PID'], sort=True)\n",
        "\n",
        "# Cleaning up duplicated rows resulted from non-1-to-1 assets mapping.\n",
        "# Cases include 1 dupe to multiple keeping assets, multiple dupes to 1 keeping asset, or multiple dupes to multiple keeping assets.\n",
        "subsetDupes = allChildMapped.duplicated(['Parent PID'], keep=False)\n",
        "\n",
        "subsetDupesCleanUp = allChildMapped[subsetDupes]\n",
        "\n",
        "# Keeping dupes that match either the filename or the checksum of a keeping asset.\n",
        "# Hyacinth's stats only counts the most downloaded assets, the rest don't matter.\n",
        "# A loop hole here is in a multiple to multiple matching, the duplicate and keeping items might share no matching files.\n",
        "# Example: The main dupe asset misses a copyright page (different checksum), and the filename different from the keeping asset.\n",
        "# Will need another parameter (e.g. download count) to do such a matching.\n",
        "\n",
        "subsetDupesCleanUp = subsetDupesCleanUp.loc[((allChildMapped['Filename_x'] == allChildMapped['Filename_y']) | (allChildMapped['Checksum_x'] == allChildMapped['Checksum_y']))]\n",
        "\n",
        "# After cleaning up duplicate matches, rejoin list of parents that have 1 child asset only.\n",
        "DupesCleaned = pd.concat([allChildMapped[~subsetDupes], subsetDupesCleanUp])\n",
        "\n",
        "# Further remove duplicates that have identical assets mapped to the same keeping asset (which still left multiple rows from last step).\n",
        "DupesCleaned.drop_duplicates(['Parent PID'], keep='first', inplace=True, ignore_index=True)\n",
        "\n",
        "# Prepare output by resetting index, selecting and renaming columns.\n",
        "DupesCleaned.sort_values(by=['Parent PID', 'PID_x'], inplace=True)\n",
        "DupesCleaned.reset_index(inplace=True, drop=True)\n",
        "DupesCleaned = DupesCleaned[['Parent PID', 'PID_x', 'Filename_x', 'PID_y', 'Filename_y']]\n",
        "DupesCleaned.columns = ['Parent PID to Keep', 'Dupe Asset PID', 'Dupe Asset Filename', 'Keeping Asset PID', 'Keeping Asset Filename']\n",
        "\n",
        "DupesCleaned.to_csv('Hyacinth_Child_Level_Mapping.csv')\n",
        "\n",
        "DupesCleaned.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe84e345",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort and export the list to work on Hyacinth\n",
        "# The first column is PID\n",
        "\n",
        "hyacinthList = allItemsMapped[['PID', 'DOI', 'Title', 'Object Type', 'Parent PID to Keep', 'DOI to Map to']]\n",
        "\n",
        "# Export as CSV file\n",
        "hyacinthList.to_csv('all_dupe_Hyacinth.csv')\n",
        "\n",
        "hyacinthList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4ed4f5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export the list to work on DataCite.\n",
        "# Those without a DOI will be removed.\n",
        "\n",
        "final_for_DataCite = allItemsMapped.dropna(subset=['DOI'])\n",
        "\n",
        "# Swapping the columns of PID and DOI\n",
        "final_for_DataCite = final_for_DataCite.reindex(columns=['DOI', 'PID', 'Object Type', 'DOI to Map to'])\n",
        "final_for_DataCite.reset_index(inplace=True, drop=True)\n",
        "\n",
        "# Export as CSV file\n",
        "final_for_DataCite.to_csv('all_dupe_Datacite.csv')\n",
        "final_for_DataCite"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "a0b92009e007ea74892ce71852617f5d08439bc7009f742fab30cbd093887492"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
